import csv
from collections import Counter

import numpy as np
import pandas as pd
import emoji
import random
import string

from nltk.stem.isri import ISRIStemmer
import nltk

from nltk.corpus import stopwords
import re
import unicodedata
from googletrans import Translator
import tweepy
from pyarabic.araby import strip_tashkeel, normalize_ligature
import ast


# df = pd.read_csv('dataset/preprocessed_tweets.csv')
# print(df_copy.head())
# print(df_copy.info())

df = pd.read_csv('dataset/Emotional-Tone-Dataset.csv')
df_copy = df.copy()

#Remove null values
df_copy.dropna(subset=[' TWEET'], inplace=True)


def clean_arabic_sentence(sentence):
    # Remove extra white spaces
    punctuations = string.punctuation + 'ØŒØ›ØŸ'
    stemmer = ISRIStemmer()

    # white space removal
    sentence = re.sub(r'\s+', ' ', sentence).strip()

    # remove punctuation
    removed_punc = ''.join(char for char in sentence if char not in punctuations)

    # remove tashkeel and elongation
    removed_tashkeel = strip_tashkeel(removed_punc)
    normalized_ligature = normalize_ligature(removed_tashkeel)
    removed_elongation = re.sub(r'(.)\1+', r'\1', normalized_ligature)

    # Tokenize the sentence into words
    words = nltk.word_tokenize(removed_elongation)

    # Stemming
    stemmed_words = [stemmer.stem(word) for word in words]

    stop_words_egyptian = ['Ø§Ù†Ø§', 'Ø§Ù†Øª', 'Ø§Ù†ØªÙŠ', 'Ø§Ù†Ù‡', 'Ø§Ù†Ù‡Ø§', 'Ù„Ø£Ù†', 'Ø§Ø²Ø§ÙŠ', 'Ø§Ø²ÙŠ', 'Ù„Ùˆ', 'Ù…Ø´',
                           'ÙƒØ¯Ø©', 'ÙƒØ¯Ù‡', 'ÙƒÙ„Ù†Ø§', 'ÙƒÙ„Ù‡', 'ÙƒÙ„Ù‡Ù…', 'ÙƒÙ„Ù‡Ù…Ø§', 'ÙƒÙ„Ù‡Ù†', 'ÙƒÙ„ÙŠ',
                           'ÙƒÙ„Ùˆ', 'Ù„ÙŠÙ‡',
                           'Ù„ÙˆØ­Ø¯Ù‡', 'Ù„ÙŠÙƒ', 'Ù„ÙŠÙ‡Ù…', 'Ù„ÙŠÙ‡Ù…Ø§', 'Ù…Ø¹Ø§Ù‡', 'Ù…Ø¹Ø§Ù‡Ø§', 'Ù…Ø¹Ø§Ù‡Ù…', 'Ù…Ø¹Ø§Ù‡Ù…Ø§', 'Ù…Ø¹Ø§Ù†Ø§', 'Ù…Ø¹Ø§Ùƒ',
                           'Ù…Ø¹Ø§Ù‡Ùˆ',
                           'Ù…Ø¹Ø§Ù‡Ù…Ùˆ', 'Ù…Ø¹Ø§Ù‡Ù†', 'Ù†Ø­Ù†Ø§', 'Ù‡Ù…Ø§', 'Ù‡Ù…Ø§Ùƒ', 'Ù‡Ù…Ø§Ù‡', 'Ù‡Ù…Ø§ÙŠØ§', 'Ù‡Ù…Ù…', 'Ù‡Ù…Ù‡', 'Ù‡Ù…Ùˆ', 'Ù‡Ù…Ù‡Ø§', 'Ù‡ÙŠ',
                           'Ù‡Ùˆ',
                           'Ù‡ÙŠØ§', 'Ù‡ÙŠÙƒ', 'Ù‡ÙŠÙƒÙ…', 'Ù‡ÙŠÙƒÙˆØ§', 'Ù‡ÙŠÙ‡Ø§Øª', 'ÙˆØ§Ù†Ø§', 'ÙˆØ§Ù†Øª', 'ÙˆØ§Ù†ØªÙŠ', 'ÙˆØ§Ù†Ù‡', 'ÙˆØ§Ù†Ù‡Ø§', 'ÙˆÙ„Ø£Ù†',
                           'ÙˆØ§Ø²Ø§ÙŠ',
                           'ÙˆØ§Ø²ÙŠ', 'ÙˆÙ„Ùˆ', 'ÙˆÙ…Ø´' 'ÙˆÙƒØ¯Ø©',
                           'ÙˆÙƒØ¯Ù‡',
                           'ÙˆÙƒÙ„Ù†Ø§', 'ÙˆÙƒÙ„Ù‡', 'ÙˆÙƒÙ„Ù‡Ù…', 'ÙˆÙƒÙ„Ù‡Ù…Ø§', 'ÙˆÙƒÙ„Ù‡Ù†', 'ÙˆÙƒÙ„ÙŠ', 'ÙˆÙƒÙ„Ùˆ', 'ÙˆÙ„ÙŠÙ‡', 'ÙˆÙ„ÙˆØ­Ø¯Ù‡', 'ÙˆÙ„ÙŠÙƒ',
                           'ÙˆÙ„ÙŠÙ‡Ù…',
                           'ÙˆÙ„ÙŠÙ‡Ù…Ø§', 'ÙˆÙ…Ø¹Ø§Ù‡', 'ÙˆÙ…Ø¹Ø§Ù‡Ø§', 'ÙˆÙ…Ø¹Ø§Ù‡Ù…', 'ÙˆÙ…Ø¹Ø§Ù‡Ù…Ø§', 'ÙˆÙ…Ø¹Ø§Ù†Ø§', 'ÙˆÙ…Ø¹Ø§Ùƒ', 'ÙˆÙ…Ø¹Ø§Ù‡Ùˆ', 'ÙˆÙ…Ø¹Ø§Ù‡Ù…Ùˆ',
                           'ÙˆÙ…Ø¹Ø§Ù‡Ù†',
                           'ÙˆÙ†Ø­Ù†Ø§', 'ÙˆÙ‡Ù…Ø§', 'ÙˆÙ‡Ù…Ø§Ùƒ', 'ÙˆÙ‡Ù…Ø§Ù‡', 'ÙˆÙ‡Ù…Ø§ÙŠØ§', 'ÙˆÙ‡Ù…Ù…', 'ÙˆÙ‡Ù…Ù‡', 'ÙˆÙ‡Ù…Ùˆ', 'ÙˆÙ‡Ù…Ù‡Ø§', 'ÙˆÙ‡ÙŠ', 'ÙˆÙ‡Ùˆ',
                           'ÙˆÙ‡ÙŠØ§',
                           'ÙˆÙ‡ÙŠÙƒ', 'ÙˆÙ‡ÙŠÙƒÙ…', 'ÙˆÙ‡ÙŠÙƒÙˆØ§', 'ÙˆÙ‡ÙŠÙ‡Ø§Øª'] + ["Ø§Ù†"] + ["Ø¹Ù„ÙŠ"] + ["Ø§Ù„ÙŠ"] + ["Ø§Ù„Ù‡"] + ["Ø§Ù†Ø§"] + [
                              "Ù…Ø´"] + ["Ø¯ÙŠ"] + ["Ø§Ù†Øª"] + ["Ø§ÙŠÙ‡"] + ["ÙˆÙ„Ù‡"] + \
                          ["Ø¯Ù‡"] + ["Ø§Ù†Ù‡"] + ["Ø§Ù„Ø§"] + ["Ø§Ù†ÙŠ"] + ["ÙƒØ¯Ù‡"] + ["Ø§ÙŠ"] + ["Ø§Ùˆ"] + ["ÙˆØ§Ù„Ù‡"] + ["Ø¹Ø´Ø§Ù†"] + [
                              "Ù…Ø¹ÙƒÙ…"] + ["ÙŠØ¹Ù†ÙŠ"] + ["Ø­ØªÙŠ"] + ["ÙƒÙ†Øª"] + ["Ø§Ø­Ù†Ø§"] + ["Ø¯Ø§"] + ["Ù„ÙŠÙ‡"] + ["ÙˆØ§Ù†Ø§"] \
                          + ["Ø§Ù†Ùƒ"] + ["Ø§Ø°Ø§"] + ["Ø²ÙŠ"] + ["Ø§Ø²Ø§ÙŠ"] + ["ÙŠÙƒÙˆÙ†"] + ["Ù…Ù†Ùƒ"] + ["ÙƒØ§Ù†Øª"] + ["Ø§Ù„"] + [
                              "ÙƒÙ„Ù‡Ø§"] + ["ÙƒÙ„Ù†Ø§"] + ["ÙƒÙ„Ù‡"] + ["Ù…Ù†Ùƒ"] + ["Ø§Ù„Ù„ÙŠ"] + \
                          ["Ù…ØµØ±"] + ["Ø§Ù„Ù„Ù‡"] + ["Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠÙ‡"] + ["Ù‚Ø·Ø±"] + ["Ø§ÙŠØ±Ø§Ù†"] + ["Ø§Ù„Ø¨Ø±Ø§Ø²ÙŠÙ„"] + ["Ø§Ù„Ù„Ù‡Ù…"] + [
                              "Ø­Ø¯"] + ["Ø­Ø§Ø¬Ù‡"] + ["d"] + ["ÙˆØ§Ù„Ù„Ù‡"]
    # ["Ø§Ù„Ø§ÙˆÙ„ÙŠÙ…Ø¨ÙŠØ§Ø¯"]
    # Remove stop words
    stop_words = stopwords.words('arabic') + stop_words_egyptian
    words = [word for word in stemmed_words if word not in stop_words]

    return words


def random_deletion(sentence):
    words = sentence.split()
    if len(words) == 1:
        return sentence
    delete_index = random.randint(0, len(words) - 1)
    del words[delete_index]
    augmented_sentence = ' '.join(words)
    return augmented_sentence


def compare_column_values(df1, df2, column_name1, column_name2):
    result = []
    if len(df1) == len(df2):
        for i in range(len(df1)):
            if df1.iloc[i][column_name1] == df2.iloc[i][column_name2]:
                result.append(True)
            else:
                result.append(False)
    else:
        result.append(False)
    return result


def convert_emojis_to_text(input_string):
    english_text = emoji.demojize(input_string, delimiters=(" ", " "))
    # print(english_text)

    translator = Translator(service_urls=['translate.google.com'])

    arabic_text = None
    try:
        arabic_text = translator.translate(english_text)
    except Exception as e:
        print(e)
    return arabic_text if arabic_text is not None else english_text


def remove_emojis(input_string):
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags
                               "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r"", input_string)


def get_tweets():
    consumer_key = 'eztIAXCyqwELFRZhWhUyRBkw1'
    consumer_secret = 'vWRigzXKNdXaMpTUMlrwZouMt2FWQC6mEADc8HOgi7LN8Hhd95'
    access_token = 'yNpkO50kGe9pFcYL9L9Lm21g9LMt40'
    access_token_secret = '9x3LKpLtA9LC20CCU8e2sQCT3JRTRDuaqH9xSp6wPQoVy'

    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_token, access_token_secret)

    api = tweepy.API(auth)

    keyword = 'ÙƒÙˆØ±ÙˆÙ†Ø§'
    language = 'ar'
    tweets = tweepy.Cursor(api.search_tweets, q=keyword, lang=language).items()

    # tweets_df = pd.DataFrame(columns=[' TWEET', 'Username', 'Timestamp'])
    for tweet in tweets:
        print(tweet.text)
        tweet_text = tweet.text
        username = tweet.user.screen_name
        timestamp = tweet.created_at

    # tweets_df = tweets_df.append({' TWEET': tweet_text, 'Username': username, 'Timestamp': timestamp},
    # ignore_index=True)

    # tweets_df.to_csv('tweets.csv', index=False, encoding='utf-8-sig')


def eliminate_speech_effect(text):
    text = strip_tashkeel(text)
    text = normalize_ligature(text)
    text = re.sub(r'(.)\1+', r'\1', text)

    return text


def findMostFrequentWords():
    df[' TWEET'] = df[' TWEET'].apply(ast.literal_eval)
    all_words = [word for tokens in df[' TWEET'] for word in tokens]
    word_freq = Counter(all_words)
    word_freq_df = pd.DataFrame.from_dict(word_freq, orient='index', columns=['Frequency'])
    word_freq_df = word_freq_df.sort_values(by='Frequency', ascending=False)

    print(word_freq)

    word_freq_list = list(word_freq.items())
    csv_file = "word_freq.csv"

    with open(csv_file, mode='w', newline='') as file:
        writer = csv.writer(file)

        for word, freq in word_freq_list:
            writer.writerow([freq, word])

    word_freq.to_csv('dataset/word_frequencies.csv', index=False)


def findMostFreqWordPerLabel():
    label_groups = df.groupby(' LABEL')

    word_freq_by_label = {}

    for label, group in label_groups:
        corpus = ' '.join(group[' TWEET'].astype(str).tolist())  # Convert values to strings before joining

        tokens = nltk.word_tokenize(corpus)  # Tokenize the text
        tokens = [word.lower() for word in tokens if word.isalpha()]  # Remove non-alphabetic tokens

        stop_words = stopwords.words('arabic') + stop_words_egyptian
        words = [word for word in tokens if word not in stop_words]

        freq_dist = nltk.FreqDist(words)
        word_freq_by_label[label] = freq_dist

    most_frequent_words_by_label = {}

    for label, freq_dist in word_freq_by_label.items():
        most_common_words = freq_dist.most_common(1)  # Get the most frequent word
        most_frequent_words_by_label[label] = most_common_words[0][0]

    print(most_frequent_words_by_label)


df_copy[' TWEET'] = df_copy[' TWEET'].apply(remove_emojis)
df_copy[' TWEET'] = df_copy[' TWEET'].apply(clean_arabic_sentence)
# same_values = compare_column_values(df_augmented, df_copy, ' TWEET', ' TWEET')

# print(df_copy[" TWEET"][539])
# print(df_augmented[" TWEET"][539])

df_copy.to_csv('dataset/preprocessed_tweets.csv', index=False)

# testing = "ÙŠØ§ Ø¯ÙŠÙ†ÙŠ Ø¹ Ø§Ù„Ø¶ÙŠÙŠÙŠÙŠÙŠÙŠÙŠÙŠÙŠÙŠØ­Ùƒ  Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡"
# testing_result = eliminate_speech_effect(testing)
# print(testing_result)

# emoji_string = "I'm feeling ðŸ¥° and ðŸ˜” at the same time"
# text_string = convert_emojis_to_text(emoji_string)
# print(text_string)

